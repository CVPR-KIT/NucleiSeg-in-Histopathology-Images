PreRun: Creating required directories
Directory Outputs/ already exists
Directory log/ already exists
PreRun: generating experiment config file
Using cuda device
configuring data loaders
loading Images from path: Dataset/trainNormal/
































































































100%|██████████| 85536/85536 [03:13<00:00, 441.96it/s]
Using cache found in /home/blue/.cache/torch/hub/facebookresearch_dinov2_main
Loading Features
Applying DBSCAN
Unique clusters: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41]
Directory Outputs/Plots/ already exists
Sampling Initialization Complete
configuring model - UNET 3+
saving model summary
logging parameters
starting training
mode: train
Epoch 0/30:   0%|          | 0/66 [00:00<?, ?it/s]
Plotting Batches for visualization
Directory Outputs/Batch_Plots/ already exists
Epoch 0/30:   0%|          | 0/66 [09:21<?, ?it/s]
Traceback (most recent call last):
  File "/home/blue/projects/MoNuSeg/main.py", line 641, in <module>
    main()
  File "/home/blue/projects/MoNuSeg/main.py", line 453, in main
    train_loss ,train_confusion_matrix, train_mIoU,train_accuracy =  run_epoch(model, train_data, criterion, optimizer, epoch, device, 'train',config)
  File "/home/blue/projects/MoNuSeg/main.py", line 131, in run_epoch
    pred = model(images.to(device))
  File "/home/blue/projects/MoNuSeg/monuSegEnv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/blue/projects/MoNuSeg/networkModules/modelUnet3p.py", line 269, in forward
    torch.cat((h1_Cat_hd1, hd2_UT_hd1, hd3_UT_hd1, hd4_UT_hd1, hd5_UT_hd1), 1)))) # hd1->320*320*UpChannels
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB (GPU 0; 47.99 GiB total capacity; 12.83 GiB already allocated; 16.71 GiB free; 13.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF